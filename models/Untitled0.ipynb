{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzDAaroPm1kX",
        "outputId": "46609d5d-b174-4961-8dba-972e31c925eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.9/dist-packages (0.3.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.9/dist-packages (from tiktoken) (2.27.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.9/dist-packages (from tiktoken) (2022.10.31)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJ1uxKlfnWmd",
        "outputId": "ee71f1a3-c692-498a-e3b6-7a65c7961ddd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Samples from tokenization: \n",
            "[b' the', b' waters', b' which', b' were', b'\\n', b'under', b' the', b' firm', b'ament', b' from', b' the', b' waters', b' which', b' were', b' above', b' the', b'\\n', b'f', b'irm', b'ame']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import tiktoken \n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "with open('gutenbergtext.txt', 'r') as f:\n",
        "    _tokens = tokenizer.encode_ordinary(f.read())\n",
        "\n",
        "# sample tokens\n",
        "print(\"\\nSamples from tokenization: \")\n",
        "print([tokenizer.decode_single_token_bytes(token) for token in _tokens[150:170]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyKCz0sqnhO-",
        "outputId": "b7175f35-5d07-43c8-982a-9e5fdd49c654"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of tokens = 52708\n"
          ]
        }
      ],
      "source": [
        "num_tokens = len(_tokens)\n",
        "vocab = list(set(_tokens))\n",
        "vocab_size = len(vocab)\n",
        "# ordinal_encodings\n",
        "\n",
        "otoe = {i : vocab[i] for i in range(vocab_size)}\n",
        "etoo = {vocab[i] : i for i in range(vocab_size)}\n",
        "# otoe = {i : _tokens[i] for i in range(num_tokens)}\n",
        "# etoo = {_tokens[i] : i for i in range(num_tokens)}\n",
        "ordinalize = lambda t : etoo[t]\n",
        "deordinalize = lambda t : otoe[t]\n",
        "\n",
        "tokens = [ordinalize(t) for t in _tokens]\n",
        "assert(_tokens == [deordinalize(t) for t in tokens])\n",
        "print(f'number of tokens = {len(tokens)}')\n",
        "assert(max(tokens) == vocab_size - 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbv2jUw8njWT"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KZ8atLbnuxr",
        "outputId": "24bef089-ed62-4b64-e732-5170177932cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device:cuda\n"
          ]
        }
      ],
      "source": [
        "batch_size = 8\n",
        "block_size = 32\n",
        "max_iters = 10000\n",
        "eval_iters = 500\n",
        "eval_interval = 500\n",
        "learning_rate = 1e-2\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "n_embd = 32\n",
        "n_head = 8\n",
        "n_layer = 8\n",
        "print(\"device:\" + device)\n",
        "dropout = 0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34oJuhUAnynO"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        # if loss is not None:\n",
        "        #     if loss < 5.5:  \n",
        "        #         learning_rate = 1e-3\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5lY91UFn6qF",
        "outputId": "03224f5f-7b8d-46c3-aa89-a827861f1cdf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([52708]) torch.int64\n"
          ]
        }
      ],
      "source": [
        "\n",
        "data = torch.tensor(tokens, dtype=torch.long,device=device)\n",
        "print(data.shape, data.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXKwROGyoEZw",
        "outputId": "ed6c3535-9166-4cd8-f78a-f42f8bd85fdf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "train_data = data[:int(num_tokens * 0.9)]\n",
        "val_data = data[int(num_tokens * 0.9): ]\n",
        "\n",
        "train_data.get_device()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78FSwnj2oXh4"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == \"train\" else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i: i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1: i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "\n",
        "xb, yb = get_batch(\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcje85ytomvN",
        "outputId": "bdabc3da-ea4b-40ad-adc2-9fa800f994ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([8, 32])\n",
            "torch.Size([8, 32])\n"
          ]
        }
      ],
      "source": [
        "print(xb.shape)\n",
        "print(yb.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNBrTPhuorRw"
      },
      "outputs": [],
      "source": [
        "\n",
        "model = BigramLanguageModel(vocab_size)\n",
        "m = model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8KOcjmUoumc"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCAOW5XBoxMr",
        "outputId": "a8aebc17-cddf-4b81-ec5e-16ac9cc7bbcf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.321262 M parameters\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfjrOAuHozAY",
        "outputId": "f2053433-54d5-449b-dd29-fd3aba34f073"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: train loss 8.2773, val loss 8.2791\n",
            "step 500: train loss 4.0666, val loss 4.7721\n",
            "step 1000: train loss 3.5686, val loss 4.7253\n",
            "step 1500: train loss 3.3250, val loss 4.7678\n",
            "step 2000: train loss 3.1547, val loss 4.7102\n",
            "step 2500: train loss 3.0557, val loss 4.7872\n",
            "step 3000: train loss 2.9688, val loss 4.7575\n",
            "step 3500: train loss 2.8325, val loss 4.7743\n",
            "step 4000: train loss 2.8177, val loss 4.7688\n",
            "step 4500: train loss 2.7339, val loss 4.8633\n",
            "step 5000: train loss 2.6566, val loss 4.8816\n",
            "step 5500: train loss 2.6221, val loss 4.8558\n",
            "step 6000: train loss 2.5902, val loss 4.8309\n",
            "step 6500: train loss 2.5403, val loss 4.9094\n",
            "step 7000: train loss 2.4820, val loss 4.9122\n",
            "step 7500: train loss 2.4661, val loss 4.9824\n",
            "step 8000: train loss 2.4457, val loss 4.9802\n",
            "step 8500: train loss 2.4022, val loss 5.0467\n",
            "step 9000: train loss 2.3610, val loss 5.0611\n",
            "step 9500: train loss 2.3686, val loss 5.0672\n",
            "step 9999: train loss 2.3364, val loss 5.0665\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "eEqKzsenpksd"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"wordModel\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hTlRcaxnpDRr",
        "outputId": "54856c6c-4faf-4f0b-d475-9acfee2cd5f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([8, 132])\n"
          ]
        }
      ],
      "source": [
        "xb, yb = get_batch('val')\n",
        "_idx = model.generate(xb, 100)\n",
        "\n",
        "print(_idx.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ObbKKATRpHOU",
        "outputId": "0baadcff-baf6-477c-8017-f8a9ad4b6b48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " between two burdens:\n",
            "And he saw that rest was good, and the land that it was\n",
            "pleasant; and bowed his shoulder to bear, and became a\n",
            "Benjamin's servants.\n",
            "And he lodged the LORD had about.\n",
            "And they took God divided the LORD God's hand of a fountain with his\n",
            "flood.\n",
            "years also he fed in the earth wrestled and their faces\n",
            "afterward, and to all the field; to be in the house of the children of the\n",
            " Bela was lift up his firstall the daughters of the Egypt; there will not they\n",
            "because they sent this manner of the cattle from thence into the\n",
            " things, that one told Joseph,\n",
            "Behold, thy father is si and he took with him his two sons,\n",
            "Manasseh and Ephra for a son: the dream, which\n",
            "wm, and looked after his sons, and they lift up, and, and\n",
            "ye pleased Joseph out of the seed house in the droves,\n",
            "cah, the Syrian, reigned in the the vale of the time of the wombs of had;\n",
            "And Abraham's son Abraham dwelt his name Beno: but Pharaoh.\n",
            "Isra butlership again not a little one to be called\n",
            "which the power with him\n",
            " money is spent; my lord also hath our herds of cattle;\n",
            "there is not ought left in the sight of my lord, but our\n",
            "bodies, and Benjamin.\n",
            "We are spies to be Joseph theirdo auled over all countries, and into the\n",
            "Sarah Jacob;\n",
            "came, he gave his sheep,\n",
            "and went from the gutters above all lands, and wept the\n",
            "bring it baker because there could not a good for them that tarry before corn, he shalt be\n",
            "not so.\n",
            "And they believed in all lands.\n",
            "And he said unto them, I came to pass with three months after your eyes,\n",
            " father's\n",
            "house, I will go up, and shew Pharaoh, and say unto him, My\n",
            "brethren, and my father's house, which shall be an.\n",
            "We are Jegarsah was risen upon his servants.\n",
            "rapel went in Gerar, that died, that Lot did\n",
            "Hagar the cities in the mount of theote God, which he had heard the hand of the had\n",
            "earth.\n",
            "And God saw them dreamed the firmament of his spirit is the\n",
            "ro Z stolen an hundred all lands.\n",
            "And the firmament the stone, that hand awoke.\n",
            "And God came into the place Let the\n",
            "escore and ten.\n",
            "And he sent Judah before him unto Joseph, to direct his face\n",
            "unto Goshen; and they came into the land of his young man's\n",
            "earth.\n",
            "And theOf sort env, which is unc thing in\n",
            "cattle, which they shall dwelled number after him on the earth be circumcised, in the ark, then, seed shall\n",
            "had Mahal to slay thyt; for he came with greatly.\n",
            "For all the name of his elder, and she arose, behold, and\n",
            "were dim,It is corn for\n",
            "and also were fulfilled, the LORD, of our raiment\n",
            " and Joseph gave\n",
            "them bread in exchange for horses, and for the flocks, and for\n",
            "the cattle of the herds, and for the ass and he put his\n",
            " Dan upon the same day.\n",
            "And he asked them, Ishmael his firstborn unto his daughter, saying,\n",
            "And Ishmael, the father, saying, Abram, Lift up from whence\n",
            "them, even to Haran, and their Israel again unto him, his sons,\n",
            "s, duke the sons which God remembered Noah an hundredth, and there it\n",
            "All he shaved up thee after theythe least men of\n",
            "Gilethineam, and\n",
            " he, and\n",
            "his seed shall become a multitude of nations.\n",
            "And he blessed them that day, saying, In thee shall Israel\n",
            "bless, saying, Wherefore benot dwell in a land. So is it Esau's house, which\n",
            "well favoured years old, that place in the flesh of the the month.\n",
            "cattle were the second month; and all that were the man increased make there after\n",
            "d themselves.\n",
            "And so began to do again a certain man asked me of all lands;\n",
            "And he said, to the God of the token of Abraham?\n",
            "God shall be light, that that lay down down to Pad\n",
            " Onan, and Shelah, and Pharez,\n",
            "and Zar but Er and Onan died in the land of Canaan. And the\n",
            "sons of Noah, Enoch said not also Eve; these were all these; tell\n",
            "And Pharaoh do this is kindly unto all food of this day.\n",
            "And when Jacob yet in his servants bre and God went in a dream before Jacob,\n",
            "camels.\n",
            "And Lot, after his wife began men, behold to call her into all Egypt.ished;\n",
            "And he came to pass, earship again the morning, Rachel, which came?\n",
            "and said, God had taken their\n",
            "armed of\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for batch in _idx:\n",
        "    res = []\n",
        "    for num in batch:\n",
        "        num2 = deordinalize(int(num))\n",
        "        res.append(num2)\n",
        "    resstr = tokenizer.decode(res)\n",
        "    print(resstr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5xtZudtCpTI9"
      },
      "outputs": [],
      "source": [
        "\n",
        "# xb, yb = get_batch('train')\n",
        "# _idx = model.generate(xb, 100)\n",
        "\n",
        "# print(_idx.shape)\n",
        "\n",
        "# for batch in _idx:\n",
        "#     res = []\n",
        "#     for num in batch:\n",
        "#         num2 = deordinalize(int(num))\n",
        "#         res.append(num2)\n",
        "#     resstr = tokenizer.decode(res)\n",
        "#     print(resstr)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzDAaroPm1kX",
        "outputId": "7d271d62-bea3-4819-cdbf-44d0e093b98c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.3.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.9/dist-packages (from tiktoken) (2022.10.31)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.9/dist-packages (from tiktoken) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.3.3\n"
          ]
        }
      ],
      "source": [
        "# !pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJ1uxKlfnWmd",
        "outputId": "8d5c98b9-f55b-4acc-9899-b5cde1f5087d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Samples from tokenization: \n",
            "[b' her', b' mother', b' are', b' credited', b' with', b' having', b' researched', b',', b'\\n', b'authent', b'icated', b',', b' and', b' compiled', b' much', b' of', b' the', b' material', b' School', b'craft']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import tiktoken \n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "with open('poetry.txt', 'r', encoding='latin-1') as f:\n",
        "    _tokens = tokenizer.encode_ordinary(f.read())\n",
        "\n",
        "# sample tokens\n",
        "print(\"\\nSamples from tokenization: \")\n",
        "print([tokenizer.decode_single_token_bytes(token) for token in _tokens[150:170]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyKCz0sqnhO-",
        "outputId": "3f19b726-759d-4003-d64f-74ad31575c7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of tokens = 4604451\n"
          ]
        }
      ],
      "source": [
        "num_tokens = len(_tokens)\n",
        "vocab = list(set(_tokens))\n",
        "vocab_size = len(vocab)\n",
        "# ordinal_encodings\n",
        "\n",
        "otoe = {i : vocab[i] for i in range(vocab_size)}\n",
        "etoo = {vocab[i] : i for i in range(vocab_size)}\n",
        "# otoe = {i : _tokens[i] for i in range(num_tokens)}\n",
        "# etoo = {_tokens[i] : i for i in range(num_tokens)}\n",
        "ordinalize = lambda t : etoo[t]\n",
        "deordinalize = lambda t : otoe[t]\n",
        "\n",
        "tokens = [ordinalize(t) for t in _tokens]\n",
        "assert(_tokens == [deordinalize(t) for t in tokens])\n",
        "print(f'number of tokens = {len(tokens)}')\n",
        "assert(max(tokens) == vocab_size - 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nbv2jUw8njWT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KZ8atLbnuxr",
        "outputId": "9fb4de43-80bc-4ccf-d6bc-45f3164cbdfe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device:cuda\n"
          ]
        }
      ],
      "source": [
        "batch_size = 16\n",
        "block_size = 32\n",
        "max_iters = 10000\n",
        "eval_iters = 500\n",
        "eval_interval = 500\n",
        "learning_rate = 1e-2\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "n_embd = 32\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "print(\"device:\" + device)\n",
        "dropout = 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "34oJuhUAnynO"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        # if loss is not None:\n",
        "        #     if loss < 5.5:  \n",
        "        #         learning_rate = 1e-3\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5lY91UFn6qF",
        "outputId": "03299c41-ee10-4b31-e150-fa79c4f3fb99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4604451]) torch.int64\n"
          ]
        }
      ],
      "source": [
        "\n",
        "data = torch.tensor(tokens, dtype=torch.long, device=device)\n",
        "print(data.shape, data.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXKwROGyoEZw",
        "outputId": "e213807b-5dd3-47ef-988f-79dd21e06ceb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "train_data = data[:int(num_tokens * 0.9)]\n",
        "val_data = data[int(num_tokens * 0.9): ]\n",
        "\n",
        "train_data.get_device()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "78FSwnj2oXh4"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == \"train\" else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i: i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1: i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "\n",
        "xb, yb = get_batch(\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcje85ytomvN",
        "outputId": "7469c27c-c778-42d2-9b00-3bfedeee6d24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([8, 32])\n",
            "torch.Size([8, 32])\n"
          ]
        }
      ],
      "source": [
        "print(xb.shape)\n",
        "print(yb.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "uNBrTPhuorRw"
      },
      "outputs": [],
      "source": [
        "\n",
        "model = BigramLanguageModel(vocab_size)\n",
        "m = model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "a8KOcjmUoumc"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCAOW5XBoxMr",
        "outputId": "00b97cdf-d6e0-4dea-90bb-0545c99286b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.06093 M parameters\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfjrOAuHozAY",
        "outputId": "2ecffbdb-7166-4f62-f2ae-163264fa401a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: train loss 5.2114, val loss 5.5830\n",
            "step 500: train loss 5.2293, val loss 5.5942\n",
            "step 1000: train loss 5.2099, val loss 5.5962\n",
            "step 1500: train loss 5.2176, val loss 5.6108\n",
            "step 2000: train loss 5.2129, val loss 5.5932\n",
            "step 2500: train loss 5.2437, val loss 5.5880\n",
            "step 3000: train loss 5.2218, val loss 5.6148\n",
            "step 3500: train loss 5.2331, val loss 5.5978\n",
            "step 4000: train loss 5.2139, val loss 5.5798\n",
            "step 4500: train loss 5.2313, val loss 5.6084\n",
            "step 5000: train loss 5.2271, val loss 5.5835\n",
            "step 5500: train loss 5.2187, val loss 5.5946\n",
            "step 6000: train loss 5.2174, val loss 5.5847\n",
            "step 6500: train loss 5.2255, val loss 5.5939\n",
            "step 7000: train loss 5.2139, val loss 5.5921\n",
            "step 7500: train loss 5.2213, val loss 5.6061\n",
            "step 8000: train loss 5.2224, val loss 5.5863\n",
            "step 8500: train loss 5.2081, val loss 5.6001\n",
            "step 9000: train loss 5.2519, val loss 5.5956\n",
            "step 9500: train loss 5.2040, val loss 5.5731\n",
            "step 9999: train loss 5.2191, val loss 5.5992\n",
            "step 0: train loss 5.1989, val loss 5.6061\n",
            "step 500: train loss 5.2078, val loss 5.5953\n",
            "step 1000: train loss 5.2239, val loss 5.6073\n",
            "step 1500: train loss 5.1950, val loss 5.5933\n",
            "step 2000: train loss 5.2258, val loss 5.6094\n",
            "step 2500: train loss 5.2092, val loss 5.6081\n",
            "step 3000: train loss 5.2131, val loss 5.5847\n",
            "step 3500: train loss 5.2006, val loss 5.6087\n",
            "step 4000: train loss 5.2003, val loss 5.5699\n",
            "step 4500: train loss 5.1928, val loss 5.5845\n",
            "step 5000: train loss 5.2215, val loss 5.6222\n",
            "step 5500: train loss 5.1961, val loss 5.6047\n",
            "step 6000: train loss 5.2034, val loss 5.6077\n",
            "step 6500: train loss 5.1969, val loss 5.6002\n",
            "step 7000: train loss 5.2082, val loss 5.5799\n",
            "step 7500: train loss 5.2145, val loss 5.6157\n",
            "step 8000: train loss 5.2042, val loss 5.5865\n",
            "step 8500: train loss 5.2469, val loss 5.6090\n",
            "step 9000: train loss 5.2185, val loss 5.5737\n",
            "step 9500: train loss 5.2412, val loss 5.6211\n",
            "step 9999: train loss 5.2270, val loss 5.6181\n",
            "step 0: train loss 5.2184, val loss 5.6175\n",
            "step 500: train loss 5.2300, val loss 5.5890\n",
            "step 1000: train loss 5.2186, val loss 5.5773\n",
            "step 1500: train loss 5.1996, val loss 5.5851\n",
            "step 2000: train loss 5.2168, val loss 5.5777\n",
            "step 2500: train loss 5.2006, val loss 5.5796\n",
            "step 3000: train loss 5.2152, val loss 5.5892\n",
            "step 3500: train loss 5.2204, val loss 5.6020\n",
            "step 4000: train loss 5.2186, val loss 5.6094\n",
            "step 4500: train loss 5.2031, val loss 5.5927\n",
            "step 5000: train loss 5.2034, val loss 5.5866\n",
            "step 5500: train loss 5.2166, val loss 5.6180\n",
            "step 6000: train loss 5.2183, val loss 5.5975\n",
            "step 6500: train loss 5.2100, val loss 5.5890\n",
            "step 7000: train loss 5.1868, val loss 5.5852\n",
            "step 7500: train loss 5.2194, val loss 5.6270\n",
            "step 8000: train loss 5.1914, val loss 5.5930\n",
            "step 8500: train loss 5.2269, val loss 5.5987\n",
            "step 9000: train loss 5.2243, val loss 5.5966\n",
            "step 9500: train loss 5.2217, val loss 5.5871\n",
            "step 9999: train loss 5.2083, val loss 5.5879\n",
            "step 0: train loss 5.1844, val loss 5.5605\n",
            "step 500: train loss 5.2297, val loss 5.6093\n",
            "step 1000: train loss 5.2078, val loss 5.6200\n",
            "step 1500: train loss 5.2050, val loss 5.6012\n",
            "step 2000: train loss 5.1812, val loss 5.6029\n",
            "step 2500: train loss 5.2040, val loss 5.6049\n",
            "step 3000: train loss 5.1951, val loss 5.5770\n",
            "step 3500: train loss 5.2117, val loss 5.6319\n",
            "step 4000: train loss 5.1969, val loss 5.5832\n",
            "step 4500: train loss 5.2311, val loss 5.6011\n",
            "step 5000: train loss 5.1993, val loss 5.6078\n",
            "step 5500: train loss 5.1886, val loss 5.5999\n",
            "step 6000: train loss 5.1930, val loss 5.5743\n",
            "step 6500: train loss 5.2036, val loss 5.6004\n",
            "step 7000: train loss 5.2053, val loss 5.6447\n",
            "step 7500: train loss 5.1966, val loss 5.5670\n",
            "step 8000: train loss 5.2045, val loss 5.5966\n",
            "step 8500: train loss 5.2012, val loss 5.6116\n",
            "step 9000: train loss 5.2116, val loss 5.6029\n",
            "step 9500: train loss 5.1794, val loss 5.5997\n",
            "step 9999: train loss 5.1937, val loss 5.5882\n",
            "step 0: train loss 5.2009, val loss 5.5890\n",
            "step 500: train loss 5.1938, val loss 5.5876\n",
            "step 1000: train loss 5.2032, val loss 5.6148\n",
            "step 1500: train loss 5.2028, val loss 5.6022\n",
            "step 2000: train loss 5.2094, val loss 5.6130\n",
            "step 2500: train loss 5.1958, val loss 5.5894\n",
            "step 3000: train loss 5.2213, val loss 5.5973\n",
            "step 3500: train loss 5.1930, val loss 5.6085\n",
            "step 4000: train loss 5.1884, val loss 5.5619\n",
            "step 4500: train loss 5.2157, val loss 5.5907\n",
            "step 5000: train loss 5.2220, val loss 5.6056\n",
            "step 5500: train loss 5.2288, val loss 5.6190\n",
            "step 6000: train loss 5.1994, val loss 5.5966\n",
            "step 6500: train loss 5.2190, val loss 5.6005\n",
            "step 7000: train loss 5.2179, val loss 5.5949\n",
            "step 7500: train loss 5.1853, val loss 5.6011\n",
            "step 8000: train loss 5.2137, val loss 5.5756\n",
            "step 8500: train loss 5.2049, val loss 5.6167\n",
            "step 9000: train loss 5.1901, val loss 5.5790\n",
            "step 9500: train loss 5.2016, val loss 5.5794\n",
            "step 9999: train loss 5.1979, val loss 5.5968\n"
          ]
        }
      ],
      "source": [
        "for x in range(5):\n",
        "    \n",
        "  for iter in range(max_iters):\n",
        "\n",
        "      # every once in a while evaluate the loss on train and val sets\n",
        "      if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "          losses = estimate_loss()\n",
        "          print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "      # sample a batch of data\n",
        "      xb, yb = get_batch('train')\n",
        "\n",
        "      # evaluate the loss\n",
        "      logits, loss = model(xb, yb)\n",
        "      optimizer.zero_grad(set_to_none=True)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "  model_name = \"Model_iter_\" + str(30000 + (x+1)*max_iters)\n",
        "  torch.save(model.state_dict(), \"Model_iter_30000\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pO7xP7cP9Wp",
        "outputId": "c4ef3c09-4e2a-4d78-8997-186ffa70f289"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "eEqKzsenpksd"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"Model_iter_80000\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTlRcaxnpDRr",
        "outputId": "891f4269-fb7c-4bb4-bf1d-e7ddb008c832"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([8, 132])\n"
          ]
        }
      ],
      "source": [
        "xb, yb = get_batch('val')\n",
        "_idx = model.generate(xb, 100)\n",
        "\n",
        "print(_idx.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObbKKATRpHOU",
        "outputId": "beae3411-035e-43bd-d922-0428d0b9d551"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "riv'd' whence in that part, where first a breach\n",
            "As of a wall appear'd, I could descry\n",
            "A portal, and three steps beneath his body on me found\n",
            "Then spied and whence they\n",
            "(Stands it's sons of my poor with the goddisedies,\n",
            "Were curves and stabbing mightier ancestry.\n",
            "Such fever, and shame denied Scouts far and seed,\n",
            "Nay the flame, they by whilst\n",
            "Of his we will ask what we twain.\n",
            "I could bras yet pad.\n",
            "In the feeling more\n",
            " sadly of unknown, that bleared to Knowledge,, all things. 1737,\n",
            "\n",
            "Your sports did determine in the month of July;\n",
            "There's less fraud in plain damme than your sly by my truly;\n",
            "'Tis sack he now serve\".\n",
            "Tune my heart is done there last. Cato dede you so,\n",
            "Or rather yet not so welessly Company with her\n",
            "Nor will much pursued, and sittingainy trees in holier summer stands.\n",
            "But wore, till the world's rich a chap and told I fell,\n",
            "Shook with the wide a grain,\n",
            "A mind with hard and wit and themselves alive,\n",
            "Of Nature and discontent amazed,\n",
            "Of his head and room to rank\n",
            "\n",
            "Let thy heels spurn the earth, and thy rais'd ken\n",
            "Fix on the lure, which heav'n's eternal King\n",
            "Whirlon!\n",
            "Oppasteth horns for little gabled with you on him, uplifted on her change not the cavalier for one sound.\n",
            "And gave I think to Is in evil a answerrying sparks\n",
            "\" hate and visageind you seed,\n",
            "He saw to direct Heaven was full brightest constellation.\n",
            "Harshiso:  Winter, who,\n",
            "That were not racing purge the lords pronounce him thought.\n",
            "And who can granted to our way!\n",
            "And there and take each\n",
            " age wert prostrate.\" --\"In that time,\n",
            "When the good Titus, with Heav'n's King to help,\n",
            "Avengle then, \"Marshes you'll willily,\" it dove,\n",
            "Poor \"Never did be brought the world, ADAM;\n",
            "Nor who shouldst do thy belly deep aEscimes now her teacher prity;\n",
            "With women have by the Eare his ramparts,remember:--.\n",
            "Har back,worn separate, they of wood behind,\n",
            "And in enow with him he did down this!\n",
            "Full on equal tongue, who been sad, and thy name\n",
            "Of\n",
            " who rolls\n",
            "In her third epicycle, shed on men\n",
            "By stream of potent radiance: therefore they\n",
            "Of elder time, in their old error blind,\n",
            "O my heart, that I kine ineffable bred:\n",
            "As yet he were, a Palestine.\n",
            "(Now rough white, but is you've sin remain?\n",
            "And shim and hope and whom Marset city:  God shall not beat,\n",
            "Afchio's defended, that bloomin'?\n",
            "They live for her eyes and bark but to please.\"\n",
            "When it not to the strength:\n",
            "But not fierce order nothing clearly sister's, yet\n",
            " twenty brass\n",
            " dream.\n",
            "It was the hour, when of diurnal heat\n",
            "No reliques chafe the cold beams of the moon,\n",
            "O'erpower'd two beyond its mighty besieged,\n",
            "And then apart from importune afawned till none Timusing her eyes.\n",
            "Unto;\n",
            "Wheeer such going to time, rest the ocean for in danger,\n",
            "And not plaint soil branch and she big treasure of battle,\n",
            "That over _And all canton battle, who\n",
            "Add all was fruitful! fading soul so great men\n",
            "That imagining did I ne' year, pattled grant greet fearorse:\n",
            "With mid the trees,\n",
            " \"O,\" they cried, \"from full hands scatter ye\n",
            "Unwith'ring lilies;\" and, so saying, cast\n",
            "Flowers over head and\n",
            "and not mine eyes andthen in my meat,\n",
            "Me of man who rein hurt--\n",
            "Shall, who struck by me, haughty laws.'\n",
            "And\n",
            "If if none, what does I have a man over land\n",
            "' onward wrings by such the Hery look\n",
            "From elements.\n",
            "Depart in vainly praise\n",
            " poison, I making!\n",
            "But iron shipum bucks that in wing,\n",
            "But a nights not help thatissa,\n",
            "Stret var\n",
            "His arms he open'd, then his wings; and spake:\n",
            "\"Onward: the steps, behold!  are near; and now\n",
            "Thou mean if 'em I think you I know not begin:\n",
            "' misfortune thought,\" but the little time thou, supplied,\n",
            "The mournroe, in the Lord: \"Let the Poet's hat \"WEDamn,\n",
            "Pene'sers.\" she deserved me Door away--\n",
            "Now tremble and fell, one whoulations sparkleft\n",
            "Nor nowiciice got aymes of him\n",
            "Lipp, and shadows away, now.\n",
            "To if thou true,\n",
            "For whom\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for batch in _idx:\n",
        "    res = []\n",
        "    for num in batch:\n",
        "        num2 = deordinalize(int(num))\n",
        "        res.append(num2)\n",
        "    resstr = tokenizer.decode(res)\n",
        "    print(resstr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5xtZudtCpTI9"
      },
      "outputs": [],
      "source": [
        "\n",
        "# xb, yb = get_batch('train')\n",
        "# _idx = model.generate(xb, 100)\n",
        "\n",
        "# print(_idx.shape)\n",
        "\n",
        "# for batch in _idx:\n",
        "#     res = []\n",
        "#     for num in batch:\n",
        "#         num2 = deordinalize(int(num))\n",
        "#         res.append(num2)\n",
        "#     resstr = tokenizer.decode(res)\n",
        "#     print(resstr)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

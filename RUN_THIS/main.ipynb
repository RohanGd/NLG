{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzDAaroPm1kX",
        "outputId": "7b6f2aeb-20b4-48a3-c103-3f7664ba1101"
      },
      "outputs": [],
      "source": [
        "# !pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hi\n"
          ]
        }
      ],
      "source": [
        "print(\"hi\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJ1uxKlfnWmd",
        "outputId": "9db65e6e-17fe-493d-8fe4-8c0a9103fdb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Samples from tokenization: \n",
            "[b' her', b' mother', b' are', b' credited', b' with', b' having', b' researched', b',', b'\\n', b'authent', b'icated', b',', b' and', b' compiled', b' much', b' of', b' the', b' material', b' School', b'craft']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import tiktoken \n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "with open('poetry.txt', 'r', encoding='latin-1') as f:\n",
        "    _tokens = tokenizer.encode_ordinary(f.read())\n",
        "\n",
        "# sample tokens\n",
        "print(\"\\nSamples from tokenization: \")\n",
        "print([tokenizer.decode_single_token_bytes(token) for token in _tokens[150:170]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyKCz0sqnhO-",
        "outputId": "465a2fae-c204-4311-f2be-517077a669d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of tokens = 5475758\n"
          ]
        }
      ],
      "source": [
        "num_tokens = len(_tokens)\n",
        "vocab = list(set(_tokens))\n",
        "vocab_size = len(vocab)\n",
        "# ordinal_encodings\n",
        "\n",
        "otoe = {i : vocab[i] for i in range(vocab_size)}\n",
        "etoo = {vocab[i] : i for i in range(vocab_size)}\n",
        "# otoe = {i : _tokens[i] for i in range(num_tokens)}\n",
        "# etoo = {_tokens[i] : i for i in range(num_tokens)}\n",
        "ordinalize = lambda t : etoo[t]\n",
        "deordinalize = lambda t : otoe[t]\n",
        "\n",
        "tokens = [ordinalize(t) for t in _tokens]\n",
        "assert(_tokens == [deordinalize(t) for t in tokens])\n",
        "print(f'number of tokens = {len(tokens)}')\n",
        "assert(max(tokens) == vocab_size - 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nbv2jUw8njWT"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\rohan\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KZ8atLbnuxr",
        "outputId": "a902888e-c654-4070-fb09-6d1d218c0c81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device:cpu\n"
          ]
        }
      ],
      "source": [
        "batch_size = 16\n",
        "block_size = 32\n",
        "max_iters = 10000\n",
        "eval_iters = 500\n",
        "eval_interval = 500\n",
        "learning_rate = 1e-2\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "n_embd = 32\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "print(\"device:\" + device)\n",
        "dropout = 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "34oJuhUAnynO"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        # if loss is not None:\n",
        "        #     if loss < 5.5:  \n",
        "        #         learning_rate = 1e-3\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5lY91UFn6qF",
        "outputId": "f59ff523-3635-470b-bccc-3605b7fb4073"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([5475758]) torch.int64\n"
          ]
        }
      ],
      "source": [
        "\n",
        "data = torch.tensor(tokens, dtype=torch.long, device=device)\n",
        "print(data.shape, data.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXKwROGyoEZw",
        "outputId": "4dfe74a4-97c3-4bb0-fa7c-c64be366cb97"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-1"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "train_data = data[:int(num_tokens * 0.9)]\n",
        "val_data = data[int(num_tokens * 0.9): ]\n",
        "\n",
        "train_data.get_device()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "78FSwnj2oXh4"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == \"train\" else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i: i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1: i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "\n",
        "xb, yb = get_batch(\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcje85ytomvN",
        "outputId": "ad78d3bb-fd73-4432-cf1e-439d2dff3276"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([16, 32])\n",
            "torch.Size([16, 32])\n"
          ]
        }
      ],
      "source": [
        "print(xb.shape)\n",
        "print(yb.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "uNBrTPhuorRw"
      },
      "outputs": [],
      "source": [
        "\n",
        "model = BigramLanguageModel(vocab_size)\n",
        "m = model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "a8KOcjmUoumc"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCAOW5XBoxMr",
        "outputId": "f11882a3-ddff-4707-e884-501b357083d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.146535 M parameters\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfjrOAuHozAY",
        "outputId": "c10f02da-3c69-43e6-91ac-3a085149b7bf"
      },
      "outputs": [],
      "source": [
        "# uncomment this to train a model from scratch\n",
        "# for x in range(5):\n",
        "  \n",
        "#   for iter in range(max_iters):\n",
        "\n",
        "#       # every once in a while evaluate the loss on train and val sets\n",
        "#       if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "#           losses = estimate_loss()\n",
        "#           print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "#       # sample a batch of dat\n",
        "#       xb, yb = get_batch('train')\n",
        "\n",
        "#       # evaluate the loss\n",
        "#       logits, loss = model(xb, yb)\n",
        "#       optimizer.zero_grad(set_to_none=True)\n",
        "#       loss.backward()\n",
        "#       optimizer.step()\n",
        "\n",
        "#   model_name = \"Model_iter_\" + str(50000 + (x+1)*max_iters)\n",
        "#   torch.save(model.state_dict(), model_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rSNZ3SZjslZ",
        "outputId": "65ab5492-3752-4810-e8e9-1da367dcc511"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "32231"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "-7PFCY596jae"
      },
      "outputs": [],
      "source": [
        "model2 = BigramLanguageModel(vocab_size)\n",
        "m2 = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pO7xP7cP9Wp",
        "outputId": "c8562bab-17ce-49c5-9d6c-dd0c8820c7c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "m2.load_state_dict(torch.load(\"Model_iter_50000 (1)\", map_location=torch.device(device)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "eEqKzsenpksd"
      },
      "outputs": [],
      "source": [
        "# to save the model after training\n",
        "# torch.save(model.state_dict(), \"Model_iter_80000\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTlRcaxnpDRr",
        "outputId": "0b6edd73-14e0-45a6-960b-3619fa724ad9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([16, 132])\n"
          ]
        }
      ],
      "source": [
        "xb, yb = get_batch('val')\n",
        "_idx = model.generate(xb, 100)\n",
        "\n",
        "print(_idx.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObbKKATRpHOU",
        "outputId": "bd28e875-1614-4bad-d826-dbe6c9064692"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " that stilled Erinna's voice.\n",
            "Ah, never with a throat that aches with song,\n",
            "Beneath the white uncaring sky of spring\n",
            "Afterakes that speech there split Ald \"Just as a Haunted goat to good house,\n",
            "And there, he broke, and down at all night reformation\n",
            "The fader Rutulchman's mad helmet tender,\n",
            "To sm next Gerinsaway, and thereby\n",
            "Is Aldobing, free to her axeth\n",
            "Resum'd Doctor asked:  how in this, depart, so fallest,\n",
            "But much halls in loud down theileen.\n",
            "And the lovely virtues who\n",
            "cted face\n",
            "The smiles of love adorn,--\n",
            "Man's inhumanity to man\n",
            "Makes countless thousands mourn!\n",
            "\"See yonder poor and th' he arising from this fourth.\n",
            "Since recompiant, and know angel bears\n",
            "To heaven the common heart's head he might purge their small memories\n",
            "DORThose fish were one tears in prime Maker, the golden shoulders, albeit with knots;\n",
            "Each three'sladen lady's pathways it they rode inside himself,\n",
            "I seemed unto the labew apace, the eighteen.\n",
            "Blood-moving round his chestthe ray,\n",
            "Of roar of thy nakedness him don in a\n",
            ",\n",
            "Far, far, behin'!\n",
            "When thou an' I were young an' skeigh,\n",
            "An' stable-meals at fair in prisoned\n",
            "But when it tempered then to be the dateless gift;\n",
            "And seem to how thy thought six I may have\n",
            "Whereof by gift you go we see you singly not one brilliantdom, careless leather.\n",
            "That After brought if it know so that the bolts tour they shore!\n",
            "Who who quickly lich asters away.\n",
            "The riche forces of the original same tract.\n",
            "One offer'd!  instructorAngel noght) With charm.\n",
            "Wum\n",
            " wrote to many a land:\n",
            "How he, who lone in Patmos banished,\n",
            "Saw in the sun a mighty angel stand,\n",
            "And heard great ingceeds. I, with any oldaunce.\n",
            "This while for Love was, one, the times, perhaps failed anger at Caesar's store.\n",
            "Tromollings, and multiplies,\n",
            "And from the noise had a blow all the flood, glade ascend, and Longolar in man,\n",
            "Goaded, 'Th battle-bed the ships, singing on the bull.\n",
            "She winsome chum fine,\n",
            "Long as I find), having told,\n",
            "The\n",
            " do not well,\n",
            "(The fools we know have their own paradise,\n",
            "The wicked also have their proper Hell);\n",
            "They have much strength but still their own sake.)\n",
            "But carpet to soothing wishes, but weeping fled she:\n",
            "A fellow to prove to run out-reach.\n",
            "They found little candy vain\n",
            "They lone Origen and made rapturing couch,\n",
            "innresses with horse into the war-\n",
            "Blue-Drops toiling stems of smiling.\n",
            " VERY all island,\n",
            "Were their Highlicts how you irco bough, a nun\n",
            "Shalt death his hands on the while low wings of everwise.\n",
            "And Agric\n",
            ", sir, find out that bee,\n",
            "Which bore my Love away.\n",
            "I'll seek him in your bonnet brave;\n",
            "I'll seek him in heart is doubledism reefs to me\n",
            "-hall-len sweet sad to Idbrother and the hand in arms\n",
            "verted flesh, \"Lain can live.\n",
            "But,\n",
            "`own baseball suck.\n",
            "To singed checcias gra Terion in the ruler of the rose and to,\n",
            "Because of speeches of stel he's on of thought.\n",
            "Than part-night, who bore that to How God thy Father,\n",
            "How baby-- wave that we hail: \n",
            " of the wood to the highway's light\n",
            "Galloped the great-limbed steed in fright;\n",
            "The mail clashed cold, and the sad owl cried far,\n",
            "This spirit.  This she died\n",
            "Voices hufels offence persuade excellent cruuted\n",
            "Schaled; but sich makes me out and this uproar,\n",
            "With charms,aught in ire their face,\n",
            "My wife. locks argue his presence fader, as in solemn carpet,\n",
            "From iron figure had received a pride of meadow poute smote\n",
            "Thatoth light his fleet-go to cleaves some same man\n",
            "Of the dust to cracklings the fire:\n",
            " surprise\n",
            "That broke my rest;\n",
            "But now a rumour's like to rise--\n",
            "A whaup's i' the nest!\n",
            "O Rough a Cash on the life and\"?\n",
            "Stepp outsider the thing gnat the lminous difficult than\n",
            "Of honor fancy squadrons brave,\n",
            "Rodom fling spear\n",
            "And dimThen, and looked into knowledge; prepare.\n",
            "ath orooming arri scares me strong\n",
            "While with him fair shepherd is moved its arms\n",
            "Of the sense so plyn, poor Mone\n",
            "Through the spilt find good Rogero-kinde smote\n",
            "Of added, my ful air, his mat\n",
            " side across the plains away\n",
            "To bear that note to \"Conroy's sheep along the Castlereagh\".\n",
            "And now by coach and mailman's bag has heard\n",
            "Known me delays and eek.\n",
            "The poised about as lived\n",
            "The ladies Grave of with, knock the rocks and glass,\n",
            "Wivid through the news, bright must be\n",
            "And smell above me\n",
            "C'sfal from their seed is why silver, wi' past the person offered to plume,\n",
            "And like ivory at the dogsask sat and a psalm remembered my treasure.-- broke.\n",
            "And to vote, and when I heard the fair grief and by\n",
            " He frowns, 'tis then ye shine!\n",
            "For my appreciation of this tribute to the poet's wife\n",
            "require a certain trim smugness and clean-for �alf the follow heav'er, or to live\n",
            "I know herself in \"I must see thee, had been keep both where recence for force,\n",
            "And him myly dressed to wits his eyes bound and hope, Marco,\n",
            "Yea said, this foul wel her gift of arms shevene\n",
            "Dere grievous his wise he with covine.\n",
            "Hast whiche acordel spare his bodi;\n",
            "Whom, upon mi Soster hou\n",
            "perides\n",
            "Shall never, scattered, in blown dust be laid,\n",
            "Till Time, the dragon-guard, has lived his last decade.\n",
            " I you talk and andthinking blinded eyes wide walls,\n",
            "Gubos for the ancient path:\n",
            "Collect, who afterwards.  .    I said none can shake the blessed,\n",
            "GON stiffened e'Twas cast afar\n",
            "That their iron laughed at a roll them last hundred corn,\n",
            "Their coming talk, urges vessels sail covered.\n",
            "And scorn take the waves north,\n",
            "TheULYOf, out of opposite hand the pictured left thee,\n",
            "With\n",
            " the church,\n",
            "Disdaining me, not seeing me.\n",
            "WHAT will you do when you come to die,\n",
            "If all your life long you have return'd.\n",
            "While sound of those hair was his grim, as each,\n",
            "Unchanged taking ghost you donned the Duck to see\n",
            "On his fury hard to I hathleg-like wrath, that I never care\n",
            "What things renew'd to fear for wary guide!  Ah me so brave bride to quivering pinnacle of each aside,\n",
            "Oppummer becomes rock it when thou!\n",
            "Except least of the, that phies!--\n",
            "Answer the Monaute; last to death had\n",
            " beneath thy potence swerving,\n",
            "'Tis that thou lead'st us by a path unknown,\n",
            "Our seeming deviations all subserving\n",
            "The perfect dawned with all his tongue-down.  And gained his titles disappears.\n",
            "Sing hihe triedst alive from his wife toods play low foe.\n",
            "There watched her Rab or its helm on the field is prepared\n",
            "Litis, another day.\n",
            " fury fall for that turn again, to tempt us question but work,\n",
            "A might should me, resembling our care not rather tread?\n",
            "Now; of the horse, as the inner palace broke\n",
            "The Danaant to wield his face\n",
            ", so maiden mild,\n",
            "But startling the close gazer with the sense\n",
            "Of passions forest-shy and forest-wild,\n",
            "And delicate delir sharp's cup before have eagles were.\n",
            "\" esteem a axe of thine effem\n",
            "There's tread a golden light and patient trees,\n",
            "The people starry man to gore, in name is thistle thank the gray sun hou fall,\n",
            "d; but the shadowy creeds al justice built close\n",
            "pointed papContents, his seated stars, away, and horns.\n",
            "He added, to them ever. rers, calmed its post--\n",
            " state, swiftly together\n",
            "ie banks of Ayr,\n",
            "Fareweel, fareweel! sweet Ballochmyle!\n",
            "Her flowing locks, the raven's wing, and mull and drinks,\n",
            "With nar the moon will mire now his hopes677,\n",
            "A battle of the boys, opposite quickly burned\n",
            "But that ever accents to me forth replied.\n",
            "For will not dear, I looked in all the rarere\n",
            "The lake flew, strong and nerves we stood the losses\n",
            "Him origh went to knowet and Adam, since,\n",
            "While I come and less not pleine dash in a rose horse.\n",
            "Of law and gife Image of\n",
            " tone.\n",
            "~I have ridden the wind,\n",
            "I have ridden the night,\n",
            "I have ridden the ghosts that flee\n",
            "From the vaults of death stand it.\n",
            "We put though the Noah.  \"Orlando,\n",
            "To thy guiltily in armour on flat snake, the expectation\n",
            "Among inMost mantled here men Alban's deathMost changed his drooping unixant from early tame.\n",
            "Among the head.\n",
            "There Muse the frail joy they divine,\n",
            "Than since opened, or soul more, wept it is evermore.\"\n",
            "Through all things that boon finds to say, who want I weep.\"\n",
            "To keep\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for batch in _idx:\n",
        "    res = []\n",
        "    for num in batch:\n",
        "        num2 = deordinalize(int(num))\n",
        "        res.append(num2)\n",
        "    resstr = tokenizer.decode(res)\n",
        "    print(resstr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "5xtZudtCpTI9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([16, 132])\n",
            ", hard as it was,\n",
            "unavenged must the atheling die!\n",
            "Too awful it is for an aged man\n",
            "to bide and bear,\n",
            "Art thou m worst is their boot, which was craven wit\n",
            "A ram the wish to changeful, lands, or Sone areving mill,\n",
            "'Saunt-nen all the music of sunshine by\n",
            "And keep the waves horse, like lamps,\n",
            "Their first seemingly thro' \"Stand thy Head with Ptried-off murmur.\n",
            "Thered gates carOn Agugian, they press her former fear and their ways,\n",
            "Of this gift this gods, was amazed\n",
            "\n",
            "Native of Thebez, wandering here, was fed\n",
            "Twice by a voice inviting him to eat.\n",
            "Of thee those forty days none hath regard\n",
            "The most amazed the one slew him ethereal deceive.\n",
            "From its.  All his foes for yearle\n",
            "This is the sense men should ensue betray the way of line,\n",
            "Of their accident manner,\n",
            "Then on the scant battle like his domains he shows\n",
            "Of earth to be rich nature grow eager eyes uncallsrecian 'What would go lif non other kinde\n",
            "And who backed axeth faire, and telle kne\n",
            "Wupuzzered my cause are\n",
            " my rural minstrelsy,\n",
            "Till fancy had her fill. But ere a close\n",
            "The wonted roar was up amidst the woods,\n",
            "Asless din and foot into the sand.\n",
            "My dark\n",
            "Which with aonscent white lighted all well fill on the land,\n",
            "Dear\n",
            "O thou hast relayed a one in mine eyes.\n",
            "Chares much crashing, here and wealth and makes thy words departing,\n",
            "Down some name, a shout, and their men smoked Clumes your wish,\n",
            "She reigns lovers bright contempliev'd host;\n",
            "Which left the Srid size of hem as fair trunk had come\n",
            "And keep the ways between these tents of thine\n",
            "And those broad seas, the seas of Palestine.\"\n",
            "From mouth to mouth the heavy rumor spread\n",
            "Of all mighty horn raimentolds the stricken home again,\n",
            "In gardens fell on young so full white streams above and.\n",
            "Despaired Helen styled the palace quarrel is too many yet but\n",
            "He held 'twoulded the kings of Policeus\n",
            "And scarce may beheld their companions upon;\n",
            "The mighty sparkled shade the riverstrip perhaps of thou never,\n",
            "And none transient conquest of just and farther hold, and their hearts delight;\n",
            "Ther work to the Greeks, mo\n",
            " do,\n",
            "E'en as Alcmaeon (who, being by his father\n",
            "Thereto entreated, his own mother slew)\n",
            "Not to retrieve the beginnings quizardoation of hem leste untrue,\n",
            "With Longulatoryeth seilter's part of allht,\n",
            "And wrong. 76, each streen that mai but esteemed,\n",
            "What is ancient arts are such illous cheer!\n",
            "\"What deem my Lord of, harassed orderets I in the alone,\n",
            "Figging could allThis sense she sprang the park!\n",
            "And he brings the lacked I'm forc'd\n",
            "My maiden group been seen the\n",
            " call,\n",
            "And looks, and thinks he hears that voice's sound,\n",
            "And thinks he sees the visage by which he\n",
            "Was so estranged from what to one,\n",
            "Excusedly genvals is the demons Powers seide.\n",
            "Lo, as it thou, a Bill hunter foot mov'd,\n",
            "May the neighbouring void the lights attent is\n",
            "Whounded her secret solitude sufficient f uncomp virtue to prom twenty)\n",
            "Old feathers'd to sing'd life confused the everybody's chiefs to hover\n",
            "With an' Lucip Southried bucklest sheven sorrows full of every hand\n",
            "Before you; of exkered visage and sooner doesn't feed\n",
            "\n",
            "His obsequies are sumptuously performed at Ravenna by Guido, who\n",
            "himself died in the ensuing year.\n",
            "I've quench Divine.\n",
            "This god's hearts of the day heven\n",
            "I have crushed suddenly the lines of castles sones but struck, who hath taken to each vain\n",
            "And ore and it convey;\n",
            "Whose adultributs heaped in spears\n",
            "To the mountain unlaced the Clutus' aect rolls the desport.\n",
            "Their passion absolute in combat thereby the rightful foemen\n",
            "With such passion lies a lion Bird\n",
            "And vitalheart, burnished soul is we never?\n",
            "Heavy\n",
            "And so would make another who in fight,\n",
            "Like me, ten opposites to death would smite.\"\n",
            "Sir Guido is besought of them to the apprehension from his capital,\n",
            "To each rest at death may see that whenbes doo wine,\n",
            "That prayers require in that you live wedding rimbly approach of disskillpe.\n",
            "He's was near high\n",
            "It then\n",
            "ev'enched myrings have prevent,\n",
            "In their white freish brace\n",
            "I keep, against her wonderful--relryments of mark\n",
            "There is some burden makes soon to fantasy regard\n",
            "Wonder strive to man who who wicked or more who have\n",
            "ser grips the bitt,\n",
            "So we'll pay you with the foresheet and a promise from the sea!\n",
            "Heh!  Tally on.\n",
            "HOM weHigh by their prophet, rewarded who be covered more to their rest;\n",
            "(Resolved to mystery hence to smell. pillars that time another sword,\n",
            "Suddenly by a coin, and resentmentphis of counselling limbs, and all has dared'd to it was none\n",
            "that by Aglimton.  Gregory of an armies thence;\n",
            "Night and suffering death to yield thee in dell.\n",
            "Is such a\n",
            "How profoundest is '--\n",
            "Is a dream to Sydney:\n",
            "Lest thus thou forcest some indeed to be\n",
            "Of an immeasurable immensity--\n",
            "Which I have taught above cannot be proved.\n",
            "And he hath left service thanne I see hastern,\n",
            "A stille he was as thus for sene\n",
            "Unt Berengas'd, as \"How surely, betide him his shield to fight\n",
            "A lot and to thenam mightease her spells, leve to seide,\n",
            "Love did my cane, as bears his obeuriate beast,\n",
            "And chiefly that then the heyseful and ten wheely Troy.\n",
            "And strives another man and caught both up, as they\n",
            "\n",
            "Are stacked in a stable; and fish, the food of desire, {1m}\n",
            "And many, but never a feast like that of the folk\n",
            "Which torn his vowberly yam spells in their Janus\n",
            "With continuous to the shield, thorns,\n",
            "Flumed'dly towns, orchard, \"Is twain performed, their flag of crimson care;\n",
            "Dan Edward Death,\n",
            "When thechard front a second entrance of hoped to the men nest;\n",
            "In any means in shame it,\n",
            "name, even already a man,\n",
            "A prayer--the mind belches, doth push'd such rare:\n",
            "As\n",
            " a peace, by costliest gifts\n",
            "Purchased, a banquet of such glad event\n",
            "Made fit memorial; and with pomp the Queen\n",
            "Displayed her to be 'ware of you swell about the person bore!\n",
            "Although you in the paramour, rivals, disputed as worse one book,\n",
            "His hand of the dire Arragon Thou doubt, to the simple way\n",
            "Think that well let me treads me is our own,\n",
            "That outled one spurred my nature teche,\n",
            "And thus they ween, and the raimhed,\n",
            "AndAlso E undoubtedly rage of towns part of the chanced,\n",
            "asks in-drop the\n",
            " his flight delays not for the view.\n",
            "Behind me I discern'd a devil black,\n",
            "That running, up advanc'd along the rock.\n",
            "Ah, untring sail with their two desires there; having taken with hate;\n",
            "And this best bark the me most yellow;\n",
            "Pcling toward the necessary refuge, to fire,\n",
            "Though tuus.  as great upon his bren parts Troy\n",
            "Is barbarous- lest does we lonely office\n",
            "Rebell from possession bear is yore ar.\n",
            "Of Italy.\" As fell him of a fond as entrance to know\n",
            "And promised interval, and gainsaean sigh with received she slew.\n",
            " hire scholden wynne\n",
            "Into the temple, and he therinne\n",
            "Schal have of hire al his entente:\n",
            "And thus nas as Crise hem foracion--,\n",
            "And from Cronique or thei before to conceived.\n",
            "Hou want appetite illustrious renderingde them, myne feaketh office,\n",
            "Which whan whin fader;\n",
            "Al thousand pai, like him mayke or for a tale,\n",
            "The little maid in large, as sufferer from the enlighten on experience.\n",
            "I've trodrown to be for thFor Reason it hate to be manifest\n",
            "Or fel m\n",
            " in the wind, and the peaks quaked in the blaze of the day\n",
            "To the hall of feasting Hiopa led them, mother and sire, who\n",
            "The hue Earth,\n",
            "Aimblade, forbids aaciously fu' gold below hope:\n",
            "Here born and escapes the judgment opened wide security, VII.,\n",
            "Flitted father, who who knew it like three.  . . .  Whereils undone the male cities sun\n",
            "Of bells of carnage--C 162 foss away. hear of gold,\n",
            "O' authors popular man who joined, firstclosed inquiring not together made way,\n",
            "All about that smigs have murdered mind\n",
            "\n",
            " a new Aurora climbs the skies,\n",
            "And from his walls Orrilo on the plain\n",
            "Drops, -- and the strife begins -- Orrilo plumed,\n",
            "Joy, after his sword was laid at' pride\n",
            "Chaddened a moist: \"Arm, his rallying thoughts,\n",
            "WhoThough spot of plenty,\n",
            "The light hath finished downward wing, gone with mere, safe repossamercious new road again, like you loved corn\n",
            "Thirty hand that-firepread allors say?\n",
            "Youin', That sees!  -- or Daniel looked from the king.\n",
            "' din was wrought!  You cannot ascend, than thus,\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# xb, yb = get_batch('train')\n",
        "# _idx = model.generate(xb, 100)\n",
        "\n",
        "# print(_idx.shape)\n",
        "\n",
        "# for batch in _idx:\n",
        "#     res = []\n",
        "#     for num in batch:\n",
        "#         num2 = deordinalize(int(num))\n",
        "#         res.append(num2)\n",
        "#     resstr = tokenizer.decode(res)\n",
        "#     print(resstr)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
